{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage import exposure\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, add, Activation, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 18057891840933340044]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = tf.compat.v1.ConfigProto(config=tf.ConfigProto(log_device_placement=True))\n",
    "# sess = tf.compat.v1.Session(config=config) \n",
    "# K.set_session(sess)\n",
    "\n",
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "# print(device_lib.list_local_devices())\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset \n",
    "https://drive.google.com/drive/u/3/folders/1sHh6NvuKX6RB5OytLwf4kaqfQ9svJNDQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "\n",
    "x_test = np.load(\"x_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# It's a multi-class classification problem \n",
    "class_index = {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
    "               'dog': 5, 'frog': 6,'horse': 7,'ship': 8, 'truck': 9}\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://img-blog.csdnimg.cn/20190623084800880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqcDE5ODcxMDEz,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert class vectors to one-hot encoding (keras model requires one-hot label as inputs)\n",
    "num_classes = 10\n",
    "print(y_train[0])\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AHE(img):\n",
    "    img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
    "    return img_adapteq\n",
    "\n",
    "# train_datagen = ImageDataGenerator()\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.125,\n",
    "    height_shift_range=0.125,\n",
    "    fill_mode='constant',\n",
    "    cval=0.,\n",
    "    rotation_range=15)\n",
    "#     preprocessing_function=AHE,\n",
    "#     rescale=1,\n",
    "#     rotation_range=15,\n",
    "#     zoom_range=0.1,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "#     fill_mode='nearest')\n",
    "    \n",
    "# train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.EarlyStopping at 0x7fd8b4cfe438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x,o_filters,increase=False):\n",
    "        stride = (1,1)\n",
    "        if increase:\n",
    "            stride = (2,2)\n",
    "\n",
    "        o1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x))\n",
    "        conv_1 = Conv2D(o_filters,kernel_size=(3,3),strides=stride,padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(o1)\n",
    "        o2  = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_1))\n",
    "        conv_2 = Conv2D(o_filters,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(o2)\n",
    "        if increase:\n",
    "            projection = Conv2D(o_filters,kernel_size=(1,1),strides=(2,2),padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(o1)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([conv_2, x])\n",
    "        return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Builde model\n",
    "# model = Sequential() # Sequential groups a linear stack of layers \n",
    "# model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=x_train.shape[1:])) # Add Convolution layers\n",
    "# model.add(Activation('relu')) # Add Relu activation for non-linearity\n",
    "# model.add(Conv2D(filters=32, kernel_size=(3, 3))) # Add Convolution layers\n",
    "# model.add(Activation('relu')) # Add Relu activation for non-linearity\n",
    "# model.add(MaxPooling2D(pool_size=(4, 4))) # Add Max pooling to lower the sptail dimension\n",
    "\n",
    "# model.add(Flatten()) # Flatten the featuremaps\n",
    "# model.add(Dense(units=512)) # Add dense layer with 512 neurons\n",
    "# model.add(Activation('relu')) # Add Relu activation for non-linearity\n",
    "# model.add(Dense(units=num_classes)) # Add final output layer for 10 classes\n",
    "# model.add(Activation('softmax')) # Add softmax activation to transfer logits into probabilities\n",
    "\n",
    "# =========\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(filters=64, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(filters=64, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "# model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "# model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(rate=0.25))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# =========\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), input_shape=x_train.shape[1:], activation =\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), input_shape=x_train.shape[1:], activation =\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), input_shape=x_train.shape[1:], activation =\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(80, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# =========\n",
    "\n",
    "stack_n            = 1\n",
    "layers             = 6 * stack_n + 2\n",
    "num_classes        = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 50000 // batch_size + 1\n",
    "weight_decay       = 1e-4\n",
    "classes_num        = 10\n",
    "\n",
    "img_input = Input(shape=(x_train.shape[1:]))\n",
    "\n",
    "x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "# input: 32x32x16 output: 32x32x16\n",
    "for _ in range(stack_n):\n",
    "    x = residual_block(x,16,False)\n",
    "\n",
    "# input: 32x32x16 output: 16x16x32\n",
    "x = residual_block(x,32,True)\n",
    "for _ in range(1,stack_n):\n",
    "    x = residual_block(x,32,False)\n",
    "\n",
    "# input: 16x16x32 output: 8x8x64\n",
    "x = residual_block(x,64,True)\n",
    "for _ in range(1,stack_n):\n",
    "    x = residual_block(x,64,False)\n",
    "\n",
    "x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# input: 64 output: 10\n",
    "x = Dense(classes_num,activation='softmax',kernel_initializer=\"he_normal\",\n",
    "          kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "\n",
    "model = Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 32)   4640        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 32)   544         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 32)   0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 64)     18496       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 64)     256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 64)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 64)     36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 64)     2112        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 64)     0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 64)     256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 64)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 78,666\n",
      "Trainable params: 78,186\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=0.005, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.9845 - accuracy: 0.2930 - val_loss: 1.8220 - val_accuracy: 0.3599\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.7165 - accuracy: 0.3847 - val_loss: 1.6224 - val_accuracy: 0.4148\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.5956 - accuracy: 0.4368 - val_loss: 1.5078 - val_accuracy: 0.4729\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.4995 - accuracy: 0.4779 - val_loss: 1.4498 - val_accuracy: 0.4926\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.4247 - accuracy: 0.5113 - val_loss: 1.4468 - val_accuracy: 0.5082\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.3650 - accuracy: 0.5340 - val_loss: 1.3008 - val_accuracy: 0.5566\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.3147 - accuracy: 0.5528 - val_loss: 1.2929 - val_accuracy: 0.5588\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.2687 - accuracy: 0.5695 - val_loss: 1.5508 - val_accuracy: 0.5075\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.2309 - accuracy: 0.5836 - val_loss: 1.4286 - val_accuracy: 0.5366\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.1946 - accuracy: 0.5984 - val_loss: 1.2231 - val_accuracy: 0.5892\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.1568 - accuracy: 0.6120 - val_loss: 1.2819 - val_accuracy: 0.5808\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.1249 - accuracy: 0.6256 - val_loss: 1.1938 - val_accuracy: 0.6197\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.0938 - accuracy: 0.6363 - val_loss: 1.2299 - val_accuracy: 0.6089\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.0726 - accuracy: 0.6449 - val_loss: 1.1919 - val_accuracy: 0.6215\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.0494 - accuracy: 0.6520 - val_loss: 1.1010 - val_accuracy: 0.6509\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.0269 - accuracy: 0.6596 - val_loss: 1.1531 - val_accuracy: 0.6370\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.0072 - accuracy: 0.6696 - val_loss: 1.1094 - val_accuracy: 0.6483\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9913 - accuracy: 0.6767 - val_loss: 1.1309 - val_accuracy: 0.6333\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9775 - accuracy: 0.6787 - val_loss: 1.2029 - val_accuracy: 0.6337\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9572 - accuracy: 0.6857 - val_loss: 1.1587 - val_accuracy: 0.6434\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9516 - accuracy: 0.6893 - val_loss: 1.0118 - val_accuracy: 0.6781\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9372 - accuracy: 0.6950 - val_loss: 1.3772 - val_accuracy: 0.5953\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9229 - accuracy: 0.6977 - val_loss: 1.0501 - val_accuracy: 0.6795\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9109 - accuracy: 0.7045 - val_loss: 1.1487 - val_accuracy: 0.6548\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.9033 - accuracy: 0.7060 - val_loss: 1.2042 - val_accuracy: 0.6430\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8888 - accuracy: 0.7122 - val_loss: 1.0946 - val_accuracy: 0.6630\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8762 - accuracy: 0.7177 - val_loss: 1.0714 - val_accuracy: 0.6657\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8677 - accuracy: 0.7199 - val_loss: 1.2764 - val_accuracy: 0.6151\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8581 - accuracy: 0.7240 - val_loss: 1.0894 - val_accuracy: 0.6661\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8498 - accuracy: 0.7256 - val_loss: 1.1013 - val_accuracy: 0.6657\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8352 - accuracy: 0.7319 - val_loss: 1.0623 - val_accuracy: 0.6794\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8284 - accuracy: 0.7335 - val_loss: 1.1151 - val_accuracy: 0.6606\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8191 - accuracy: 0.7384 - val_loss: 0.9296 - val_accuracy: 0.7101\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8091 - accuracy: 0.7400 - val_loss: 0.9938 - val_accuracy: 0.6922\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.8048 - accuracy: 0.7423 - val_loss: 1.0091 - val_accuracy: 0.6956\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7910 - accuracy: 0.7491 - val_loss: 0.9714 - val_accuracy: 0.7023\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7827 - accuracy: 0.7514 - val_loss: 1.0105 - val_accuracy: 0.7059\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7808 - accuracy: 0.7516 - val_loss: 0.8545 - val_accuracy: 0.7408\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7746 - accuracy: 0.7540 - val_loss: 0.9058 - val_accuracy: 0.7235\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7652 - accuracy: 0.7584 - val_loss: 0.8451 - val_accuracy: 0.7444\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7523 - accuracy: 0.7633 - val_loss: 0.8887 - val_accuracy: 0.7314\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7518 - accuracy: 0.7613 - val_loss: 0.9387 - val_accuracy: 0.7155\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7429 - accuracy: 0.7653 - val_loss: 0.9064 - val_accuracy: 0.7308\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7346 - accuracy: 0.7685 - val_loss: 0.9119 - val_accuracy: 0.7338\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7299 - accuracy: 0.7715 - val_loss: 0.8223 - val_accuracy: 0.7504\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7213 - accuracy: 0.7723 - val_loss: 0.9292 - val_accuracy: 0.7269\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7152 - accuracy: 0.7758 - val_loss: 0.8430 - val_accuracy: 0.7454\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7115 - accuracy: 0.7768 - val_loss: 1.0327 - val_accuracy: 0.7030\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7091 - accuracy: 0.7787 - val_loss: 0.9053 - val_accuracy: 0.7418\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.7038 - accuracy: 0.7793 - val_loss: 0.9113 - val_accuracy: 0.7338\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6954 - accuracy: 0.7831 - val_loss: 0.8005 - val_accuracy: 0.7563\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6947 - accuracy: 0.7826 - val_loss: 0.8453 - val_accuracy: 0.7512\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6899 - accuracy: 0.7849 - val_loss: 0.8029 - val_accuracy: 0.7623\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6869 - accuracy: 0.7864 - val_loss: 0.8573 - val_accuracy: 0.7497\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6761 - accuracy: 0.7912 - val_loss: 0.9052 - val_accuracy: 0.7348\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6783 - accuracy: 0.7905 - val_loss: 0.7916 - val_accuracy: 0.7605\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6738 - accuracy: 0.7902 - val_loss: 0.8075 - val_accuracy: 0.7574\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6671 - accuracy: 0.7924 - val_loss: 0.8357 - val_accuracy: 0.7579\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6679 - accuracy: 0.7930 - val_loss: 0.8375 - val_accuracy: 0.7524\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6633 - accuracy: 0.7939 - val_loss: 0.9228 - val_accuracy: 0.7357\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6551 - accuracy: 0.7960 - val_loss: 0.9170 - val_accuracy: 0.7382\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6540 - accuracy: 0.7969 - val_loss: 0.7813 - val_accuracy: 0.7615\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6498 - accuracy: 0.7994 - val_loss: 0.7624 - val_accuracy: 0.7742\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6472 - accuracy: 0.8000 - val_loss: 0.8038 - val_accuracy: 0.7657\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6465 - accuracy: 0.8024 - val_loss: 0.8256 - val_accuracy: 0.7559\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6443 - accuracy: 0.8005 - val_loss: 0.8490 - val_accuracy: 0.7555\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6399 - accuracy: 0.8029 - val_loss: 0.7782 - val_accuracy: 0.7700\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6333 - accuracy: 0.8076 - val_loss: 0.8153 - val_accuracy: 0.7593\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6310 - accuracy: 0.8059 - val_loss: 0.8537 - val_accuracy: 0.7482\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6317 - accuracy: 0.8057 - val_loss: 0.8276 - val_accuracy: 0.7572\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6273 - accuracy: 0.8064 - val_loss: 0.8444 - val_accuracy: 0.7558\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6270 - accuracy: 0.8065 - val_loss: 0.7401 - val_accuracy: 0.7809\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6219 - accuracy: 0.8069 - val_loss: 0.7453 - val_accuracy: 0.7829\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6195 - accuracy: 0.8098 - val_loss: 0.7416 - val_accuracy: 0.7815\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6142 - accuracy: 0.8128 - val_loss: 0.8254 - val_accuracy: 0.7637\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6129 - accuracy: 0.8128 - val_loss: 0.8245 - val_accuracy: 0.7655\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6113 - accuracy: 0.8118 - val_loss: 0.7182 - val_accuracy: 0.7868\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6083 - accuracy: 0.8132 - val_loss: 0.8075 - val_accuracy: 0.7676\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6043 - accuracy: 0.8148 - val_loss: 0.8038 - val_accuracy: 0.7632\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6042 - accuracy: 0.8161 - val_loss: 0.8624 - val_accuracy: 0.7575\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.6032 - accuracy: 0.8152 - val_loss: 0.7915 - val_accuracy: 0.7717\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5990 - accuracy: 0.8161 - val_loss: 0.8044 - val_accuracy: 0.7672\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5935 - accuracy: 0.8188 - val_loss: 0.7696 - val_accuracy: 0.7766\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5920 - accuracy: 0.8197 - val_loss: 0.7802 - val_accuracy: 0.7785\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5913 - accuracy: 0.8189 - val_loss: 0.6517 - val_accuracy: 0.8041\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5922 - accuracy: 0.8206 - val_loss: 0.8180 - val_accuracy: 0.7631\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5887 - accuracy: 0.8219 - val_loss: 0.8775 - val_accuracy: 0.7502\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5871 - accuracy: 0.8205 - val_loss: 0.7545 - val_accuracy: 0.7782\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5855 - accuracy: 0.8223 - val_loss: 0.7234 - val_accuracy: 0.7923\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5793 - accuracy: 0.8257 - val_loss: 0.7052 - val_accuracy: 0.7984\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5804 - accuracy: 0.8250 - val_loss: 0.7281 - val_accuracy: 0.7859\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5800 - accuracy: 0.8221 - val_loss: 0.7162 - val_accuracy: 0.7933\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5734 - accuracy: 0.8257 - val_loss: 0.6881 - val_accuracy: 0.7947\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5750 - accuracy: 0.8247 - val_loss: 0.7660 - val_accuracy: 0.7820\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5703 - accuracy: 0.8281 - val_loss: 0.7256 - val_accuracy: 0.7907\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5700 - accuracy: 0.8268 - val_loss: 0.7303 - val_accuracy: 0.7827\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5719 - accuracy: 0.8264 - val_loss: 0.7895 - val_accuracy: 0.7761\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5628 - accuracy: 0.8303 - val_loss: 0.7156 - val_accuracy: 0.7946\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5684 - accuracy: 0.8275 - val_loss: 0.6607 - val_accuracy: 0.8041\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5657 - accuracy: 0.8299 - val_loss: 0.7999 - val_accuracy: 0.7684\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5600 - accuracy: 0.8295 - val_loss: 0.7532 - val_accuracy: 0.7838\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5604 - accuracy: 0.8318 - val_loss: 0.6521 - val_accuracy: 0.8076\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5644 - accuracy: 0.8310 - val_loss: 0.8268 - val_accuracy: 0.7572\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5618 - accuracy: 0.8300 - val_loss: 0.7544 - val_accuracy: 0.7839\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5536 - accuracy: 0.8307 - val_loss: 0.8193 - val_accuracy: 0.7628\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5549 - accuracy: 0.8320 - val_loss: 0.7113 - val_accuracy: 0.7925\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5544 - accuracy: 0.8314 - val_loss: 0.7284 - val_accuracy: 0.7892\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5476 - accuracy: 0.8330 - val_loss: 0.6831 - val_accuracy: 0.8023\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5544 - accuracy: 0.8326 - val_loss: 0.7170 - val_accuracy: 0.7919\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5478 - accuracy: 0.8334 - val_loss: 0.7513 - val_accuracy: 0.7854\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5493 - accuracy: 0.8334 - val_loss: 0.6766 - val_accuracy: 0.8047\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5462 - accuracy: 0.8355 - val_loss: 0.7083 - val_accuracy: 0.7885\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5467 - accuracy: 0.8337 - val_loss: 0.8580 - val_accuracy: 0.7631\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5468 - accuracy: 0.8354 - val_loss: 0.7426 - val_accuracy: 0.7842\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5408 - accuracy: 0.8355 - val_loss: 0.6711 - val_accuracy: 0.8042\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5455 - accuracy: 0.8353 - val_loss: 0.7614 - val_accuracy: 0.7829\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 85s 219ms/step - loss: 0.5384 - accuracy: 0.8374 - val_loss: 0.6740 - val_accuracy: 0.8048\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5395 - accuracy: 0.8363 - val_loss: 0.6722 - val_accuracy: 0.8037\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5386 - accuracy: 0.8391 - val_loss: 0.6247 - val_accuracy: 0.8155\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5324 - accuracy: 0.8406 - val_loss: 0.6530 - val_accuracy: 0.8108\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5339 - accuracy: 0.8410 - val_loss: 0.6883 - val_accuracy: 0.7983\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5338 - accuracy: 0.8419 - val_loss: 0.6695 - val_accuracy: 0.8051\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5336 - accuracy: 0.8388 - val_loss: 0.6987 - val_accuracy: 0.7981\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5285 - accuracy: 0.8415 - val_loss: 0.6903 - val_accuracy: 0.8007\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5284 - accuracy: 0.8407 - val_loss: 0.7371 - val_accuracy: 0.7894\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5256 - accuracy: 0.8418 - val_loss: 0.8012 - val_accuracy: 0.7644\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5279 - accuracy: 0.8412 - val_loss: 0.6732 - val_accuracy: 0.8046\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5254 - accuracy: 0.8416 - val_loss: 0.7637 - val_accuracy: 0.7822\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5259 - accuracy: 0.8414 - val_loss: 0.6837 - val_accuracy: 0.8045\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5243 - accuracy: 0.8422 - val_loss: 0.6907 - val_accuracy: 0.8032\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5219 - accuracy: 0.8428 - val_loss: 0.7552 - val_accuracy: 0.7864\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5243 - accuracy: 0.8415 - val_loss: 0.6803 - val_accuracy: 0.8023\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5212 - accuracy: 0.8437 - val_loss: 0.7232 - val_accuracy: 0.7938\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5213 - accuracy: 0.8426 - val_loss: 0.6888 - val_accuracy: 0.8024\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5209 - accuracy: 0.8438 - val_loss: 0.7721 - val_accuracy: 0.7838\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5192 - accuracy: 0.8447 - val_loss: 0.6870 - val_accuracy: 0.8066\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5170 - accuracy: 0.8455 - val_loss: 0.6655 - val_accuracy: 0.8027\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5169 - accuracy: 0.8467 - val_loss: 0.6969 - val_accuracy: 0.7947\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5138 - accuracy: 0.8463 - val_loss: 0.7294 - val_accuracy: 0.7950\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5139 - accuracy: 0.8452 - val_loss: 0.6697 - val_accuracy: 0.8076\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5143 - accuracy: 0.8456 - val_loss: 0.6822 - val_accuracy: 0.7987\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5088 - accuracy: 0.8475 - val_loss: 0.7059 - val_accuracy: 0.7949\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5098 - accuracy: 0.8483 - val_loss: 0.6645 - val_accuracy: 0.8047\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5121 - accuracy: 0.8475 - val_loss: 0.8053 - val_accuracy: 0.7812\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5059 - accuracy: 0.8474 - val_loss: 0.6606 - val_accuracy: 0.8100\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5049 - accuracy: 0.8506 - val_loss: 0.7183 - val_accuracy: 0.7893\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5095 - accuracy: 0.8460 - val_loss: 0.6486 - val_accuracy: 0.8107\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5034 - accuracy: 0.8492 - val_loss: 0.6821 - val_accuracy: 0.8038\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5018 - accuracy: 0.8506 - val_loss: 0.6747 - val_accuracy: 0.8097\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5032 - accuracy: 0.8503 - val_loss: 0.6865 - val_accuracy: 0.8067\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5079 - accuracy: 0.8488 - val_loss: 0.6790 - val_accuracy: 0.8055\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5028 - accuracy: 0.8492 - val_loss: 0.6877 - val_accuracy: 0.8029\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5011 - accuracy: 0.8504 - val_loss: 0.6691 - val_accuracy: 0.8086\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4976 - accuracy: 0.8510 - val_loss: 0.6763 - val_accuracy: 0.8044\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4967 - accuracy: 0.8524 - val_loss: 0.7429 - val_accuracy: 0.7956\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.5009 - accuracy: 0.8510 - val_loss: 0.6975 - val_accuracy: 0.8030\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4985 - accuracy: 0.8513 - val_loss: 0.7061 - val_accuracy: 0.8000\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4970 - accuracy: 0.8517 - val_loss: 0.7414 - val_accuracy: 0.7924\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4980 - accuracy: 0.8523 - val_loss: 0.6715 - val_accuracy: 0.8090\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4975 - accuracy: 0.8515 - val_loss: 0.6864 - val_accuracy: 0.8011\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4925 - accuracy: 0.8559 - val_loss: 0.7901 - val_accuracy: 0.7766\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4935 - accuracy: 0.8527 - val_loss: 0.6436 - val_accuracy: 0.8135\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4895 - accuracy: 0.8543 - val_loss: 0.6809 - val_accuracy: 0.8047\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4922 - accuracy: 0.8526 - val_loss: 0.6529 - val_accuracy: 0.8139\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4926 - accuracy: 0.8526 - val_loss: 0.5886 - val_accuracy: 0.8307\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4886 - accuracy: 0.8562 - val_loss: 0.6671 - val_accuracy: 0.8106\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4918 - accuracy: 0.8540 - val_loss: 0.6386 - val_accuracy: 0.8162\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4862 - accuracy: 0.8544 - val_loss: 0.7057 - val_accuracy: 0.7979\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4885 - accuracy: 0.8564 - val_loss: 0.6896 - val_accuracy: 0.8025\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4855 - accuracy: 0.8564 - val_loss: 0.6955 - val_accuracy: 0.8086\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4824 - accuracy: 0.8579 - val_loss: 0.6734 - val_accuracy: 0.8056\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4835 - accuracy: 0.8559 - val_loss: 0.7144 - val_accuracy: 0.8005\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4860 - accuracy: 0.8567 - val_loss: 0.6516 - val_accuracy: 0.8089\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4800 - accuracy: 0.8575 - val_loss: 0.6262 - val_accuracy: 0.8176\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4864 - accuracy: 0.8548 - val_loss: 0.7169 - val_accuracy: 0.7978\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4838 - accuracy: 0.8560 - val_loss: 0.6257 - val_accuracy: 0.8196\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4826 - accuracy: 0.8578 - val_loss: 0.6378 - val_accuracy: 0.8207\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4813 - accuracy: 0.8578 - val_loss: 0.7228 - val_accuracy: 0.7971\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4824 - accuracy: 0.8567 - val_loss: 0.6536 - val_accuracy: 0.8140\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4794 - accuracy: 0.8582 - val_loss: 0.7297 - val_accuracy: 0.8036\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4757 - accuracy: 0.8597 - val_loss: 0.7478 - val_accuracy: 0.7936\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4781 - accuracy: 0.8592 - val_loss: 0.7451 - val_accuracy: 0.7958\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 85s 219ms/step - loss: 0.4753 - accuracy: 0.8619 - val_loss: 0.6157 - val_accuracy: 0.8268\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4813 - accuracy: 0.8584 - val_loss: 0.7067 - val_accuracy: 0.7951\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4772 - accuracy: 0.8585 - val_loss: 0.6479 - val_accuracy: 0.8109\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4737 - accuracy: 0.8608 - val_loss: 0.6347 - val_accuracy: 0.8139\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4768 - accuracy: 0.8583 - val_loss: 0.6816 - val_accuracy: 0.8087\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4757 - accuracy: 0.8607 - val_loss: 0.6491 - val_accuracy: 0.8162\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4726 - accuracy: 0.8606 - val_loss: 0.6685 - val_accuracy: 0.8116\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4743 - accuracy: 0.8597 - val_loss: 0.7273 - val_accuracy: 0.7967\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4779 - accuracy: 0.8586 - val_loss: 0.7457 - val_accuracy: 0.7938\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4725 - accuracy: 0.8615 - val_loss: 0.7033 - val_accuracy: 0.7979\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4740 - accuracy: 0.8593 - val_loss: 0.5989 - val_accuracy: 0.8264\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4743 - accuracy: 0.8597 - val_loss: 0.5795 - val_accuracy: 0.8311\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4697 - accuracy: 0.8628 - val_loss: 0.6719 - val_accuracy: 0.8082\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4721 - accuracy: 0.8606 - val_loss: 0.6497 - val_accuracy: 0.8175\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4669 - accuracy: 0.8637 - val_loss: 0.6612 - val_accuracy: 0.8162\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4669 - accuracy: 0.8619 - val_loss: 0.6769 - val_accuracy: 0.8103\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4649 - accuracy: 0.8629 - val_loss: 0.6337 - val_accuracy: 0.8201\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.4651 - accuracy: 0.8637 - val_loss: 0.6713 - val_accuracy: 0.8096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7c2748fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with loss function and optimizer, and evaluate with accuracy\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Setup some hyperparameters\n",
    "# batch_size = 32\n",
    "# epochs = 100\n",
    "\n",
    "# Fit the data into model\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_test, y_test),\n",
    "#           shuffle=True)\n",
    "\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=iterations,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(y_pred.shape) # 10000 samples, each sample with probaility of 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.81414742e-07, 9.94904757e-01, 1.16518947e-10, 1.09749344e-07,\n",
       "       5.31831038e-11, 1.43482684e-10, 1.13031329e-12, 7.58477459e-09,\n",
       "       9.55867776e-08, 5.09477640e-03], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred[0]) # argmax to find the predict class with highest probability. 9=truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT MODIFY CODE BELOW!\n",
    "**Please screen shot your results and post it on your report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (10000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of my model on test set:  0.8096\n"
     ]
    }
   ],
   "source": [
    "y_test = np.load(\"y_test.npy\")\n",
    "print(\"Accuracy of my model on test set: \", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
